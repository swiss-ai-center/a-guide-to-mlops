{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a303b4",
   "metadata": {},
   "source": [
    "# R-programming classification\n",
    "\n",
    "A ML model that can classify text into two categories: \"related to R programming language\" and \"not\n",
    "related to R programming language\".\n",
    "The dataset consists of 10,000 posts from \n",
    "StackOverflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38958551",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "024eff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from dvclive import Live\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b778dc3f",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa8e6f-3bba-4cd7-a473-75c32ff6320c",
   "metadata": {},
   "source": [
    "Experiment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "791fb6d1-ec09-4a5d-ab08-80fb4680b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_prepare_split = 0.20\n",
    "params_prepare_seed = 20170428\n",
    "\n",
    "params_featurize_max_features = 100\n",
    "params_featurize_ngrams = 1\n",
    "\n",
    "params_train_seed = 20170428\n",
    "params_train_n_est = 50\n",
    "params_train_min_split = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace7638b",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c86d14e-b20a-41ff-b942-48df407455ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = sys.argv[1]\n",
    "output_train = os.path.join(\"data\", \"prepared\", \"train.tsv\")\n",
    "output_test = os.path.join(\"data\", \"prepared\", \"test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "206145ce-4be2-4259-96ec-d82d71636b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data set split ratio\n",
    "split = params_prepare_split\n",
    "random.seed(params_prepare_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c6a8e05-ce17-403c-8417-7c851b0468e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m             sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping the broken line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprepared\u001b[39m\u001b[38;5;124m\"\u001b[39m), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fd_in:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(output_train, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fd_out_train:\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(output_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fd_out_test:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '-f'"
     ]
    }
   ],
   "source": [
    "def process_posts(fd_in, fd_out_train, fd_out_test, target_tag):\n",
    "    num = 1\n",
    "    for line in fd_in:\n",
    "        try:\n",
    "            fd_out = fd_out_train if random.random() > split else fd_out_test\n",
    "            attr = xml.etree.ElementTree.fromstring(line).attrib\n",
    "\n",
    "            pid = attr.get(\"Id\", \"\")\n",
    "            label = 1 if target_tag in attr.get(\"Tags\", \"\") else 0\n",
    "            title = re.sub(r\"\\s+\", \" \", attr.get(\"Title\", \"\")).strip()\n",
    "            body = re.sub(r\"\\s+\", \" \", attr.get(\"Body\", \"\")).strip()\n",
    "            text = title + \" \" + body\n",
    "\n",
    "            fd_out.write(\"{}\\t{}\\t{}\\n\".format(pid, label, text))\n",
    "\n",
    "            num += 1\n",
    "        except Exception as ex:\n",
    "            sys.stderr.write(f\"Skipping the broken line {num}: {ex}\\n\")\n",
    "\n",
    "os.makedirs(os.path.join(\"data\", \"prepared\"), exist_ok=True)\n",
    "\n",
    "with io.open(input, encoding=\"utf8\") as fd_in:\n",
    "    with io.open(output_train, \"w\", encoding=\"utf8\") as fd_out_train:\n",
    "        with io.open(output_test, \"w\", encoding=\"utf8\") as fd_out_test:\n",
    "            process_posts(fd_in, fd_out_train, fd_out_test, \"<r>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e923c-0d5c-4eba-945b-4478fed2f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import xml.etree.ElementTree\n",
    "\n",
    "import yaml\n",
    "\n",
    "params = yaml.safe_load(open(\"params.yaml\"))[\"prepare\"]\n",
    "\n",
    "if len(sys.argv) != 2:\n",
    "    sys.stderr.write(\"Arguments error. Usage:\\n\")\n",
    "    sys.stderr.write(\"\\tpython prepare.py data-file\\n\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Test data set split ratio\n",
    "split = params[\"split\"]\n",
    "random.seed(params[\"seed\"])\n",
    "\n",
    "input = sys.argv[1]\n",
    "output_train = os.path.join(\"data\", \"prepared\", \"train.tsv\")\n",
    "output_test = os.path.join(\"data\", \"prepared\", \"test.tsv\")\n",
    "\n",
    "\n",
    "def process_posts(fd_in, fd_out_train, fd_out_test, target_tag):\n",
    "    num = 1\n",
    "    for line in fd_in:\n",
    "        try:\n",
    "            fd_out = fd_out_train if random.random() > split else fd_out_test\n",
    "            attr = xml.etree.ElementTree.fromstring(line).attrib\n",
    "\n",
    "            pid = attr.get(\"Id\", \"\")\n",
    "            label = 1 if target_tag in attr.get(\"Tags\", \"\") else 0\n",
    "            title = re.sub(r\"\\s+\", \" \", attr.get(\"Title\", \"\")).strip()\n",
    "            body = re.sub(r\"\\s+\", \" \", attr.get(\"Body\", \"\")).strip()\n",
    "            text = title + \" \" + body\n",
    "\n",
    "            fd_out.write(\"{}\\t{}\\t{}\\n\".format(pid, label, text))\n",
    "\n",
    "            num += 1\n",
    "        except Exception as ex:\n",
    "            sys.stderr.write(f\"Skipping the broken line {num}: {ex}\\n\")\n",
    "\n",
    "\n",
    "os.makedirs(os.path.join(\"data\", \"prepared\"), exist_ok=True)\n",
    "\n",
    "with io.open(input, encoding=\"utf8\") as fd_in:\n",
    "    with io.open(output_train, \"w\", encoding=\"utf8\") as fd_out_train:\n",
    "        with io.open(output_test, \"w\", encoding=\"utf8\") as fd_out_test:\n",
    "            process_posts(fd_in, fd_out_train, fd_out_test, \"<r>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4217ad08",
   "metadata": {},
   "source": [
    "## Featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7315f3-6f4d-46f0-9daf-cc74fb24e286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sparse\n",
    "import yaml\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "params = yaml.safe_load(open(\"params.yaml\"))[\"featurize\"]\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "if len(sys.argv) != 3 and len(sys.argv) != 5:\n",
    "    sys.stderr.write(\"Arguments error. Usage:\\n\")\n",
    "    sys.stderr.write(\"\\tpython featurization.py data-dir-path features-dir-path\\n\")\n",
    "    sys.exit(1)\n",
    "\n",
    "train_input = os.path.join(sys.argv[1], \"train.tsv\")\n",
    "test_input = os.path.join(sys.argv[1], \"test.tsv\")\n",
    "train_output = os.path.join(sys.argv[2], \"train.pkl\")\n",
    "test_output = os.path.join(sys.argv[2], \"test.pkl\")\n",
    "\n",
    "max_features = params[\"max_features\"]\n",
    "ngrams = params[\"ngrams\"]\n",
    "\n",
    "\n",
    "def get_df(data):\n",
    "    df = pd.read_csv(\n",
    "        data,\n",
    "        encoding=\"utf-8\",\n",
    "        header=None,\n",
    "        delimiter=\"\\t\",\n",
    "        names=[\"id\", \"label\", \"text\"],\n",
    "    )\n",
    "    sys.stderr.write(f\"The input data frame {data} size is {df.shape}\\n\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_matrix(df, matrix, names, output):\n",
    "    id_matrix = sparse.csr_matrix(df.id.astype(np.int64)).T\n",
    "    label_matrix = sparse.csr_matrix(df.label.astype(np.int64)).T\n",
    "\n",
    "    result = sparse.hstack([id_matrix, label_matrix, matrix], format=\"csr\")\n",
    "\n",
    "    msg = \"The output matrix {} size is {} and data type is {}\\n\"\n",
    "    sys.stderr.write(msg.format(output, result.shape, result.dtype))\n",
    "\n",
    "    with open(output, \"wb\") as fd:\n",
    "        pickle.dump((result, names), fd)\n",
    "    pass\n",
    "\n",
    "\n",
    "os.makedirs(sys.argv[2], exist_ok=True)\n",
    "\n",
    "# Generate train feature matrix\n",
    "df_train = get_df(train_input)\n",
    "train_words = np.array(df_train.text.str.lower().values.astype(\"U\"))\n",
    "\n",
    "bag_of_words = CountVectorizer(\n",
    "    stop_words=\"english\", max_features=max_features, ngram_range=(1, ngrams)\n",
    ")\n",
    "\n",
    "bag_of_words.fit(train_words)\n",
    "train_words_binary_matrix = bag_of_words.transform(train_words)\n",
    "feature_names = bag_of_words.get_feature_names_out()\n",
    "tfidf = TfidfTransformer(smooth_idf=False)\n",
    "tfidf.fit(train_words_binary_matrix)\n",
    "train_words_tfidf_matrix = tfidf.transform(train_words_binary_matrix)\n",
    "\n",
    "save_matrix(df_train, train_words_tfidf_matrix, feature_names, train_output)\n",
    "\n",
    "# Generate test feature matrix\n",
    "df_test = get_df(test_input)\n",
    "test_words = np.array(df_test.text.str.lower().values.astype(\"U\"))\n",
    "test_words_binary_matrix = bag_of_words.transform(test_words)\n",
    "test_words_tfidf_matrix = tfidf.transform(test_words_binary_matrix)\n",
    "\n",
    "save_matrix(df_test, test_words_tfidf_matrix, feature_names, test_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2545a0a",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a30498-4d38-4c72-bd21-23c839c181e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "params = yaml.safe_load(open(\"params.yaml\"))[\"train\"]\n",
    "\n",
    "if len(sys.argv) != 3:\n",
    "    sys.stderr.write(\"Arguments error. Usage:\\n\")\n",
    "    sys.stderr.write(\"\\tpython train.py features model\\n\")\n",
    "    sys.exit(1)\n",
    "\n",
    "input = sys.argv[1]\n",
    "output = sys.argv[2]\n",
    "seed = params[\"seed\"]\n",
    "n_est = params[\"n_est\"]\n",
    "min_split = params[\"min_split\"]\n",
    "\n",
    "with open(os.path.join(input, \"train.pkl\"), \"rb\") as fd:\n",
    "    matrix, _ = pickle.load(fd)\n",
    "\n",
    "labels = np.squeeze(matrix[:, 1].toarray())\n",
    "x = matrix[:, 2:]\n",
    "\n",
    "sys.stderr.write(\"Input matrix size {}\\n\".format(matrix.shape))\n",
    "sys.stderr.write(\"X matrix size {}\\n\".format(x.shape))\n",
    "sys.stderr.write(\"Y matrix size {}\\n\".format(labels.shape))\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=n_est, min_samples_split=min_split, n_jobs=2, random_state=seed\n",
    ")\n",
    "\n",
    "clf.fit(x, labels)\n",
    "\n",
    "with open(output, \"wb\") as fd:\n",
    "    pickle.dump(clf, fd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5faf12a",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360e39c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from dvclive import Live\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "if len(sys.argv) != 3:\n",
    "    sys.stderr.write(\"Arguments error. Usage:\\n\")\n",
    "    sys.stderr.write(\"\\tpython evaluate.py model features\\n\")\n",
    "    sys.exit(1)\n",
    "\n",
    "model_file = sys.argv[1]\n",
    "matrix_file = os.path.join(sys.argv[2], \"test.pkl\")\n",
    "\n",
    "with open(model_file, \"rb\") as fd:\n",
    "    model = pickle.load(fd)\n",
    "\n",
    "with open(matrix_file, \"rb\") as fd:\n",
    "    matrix, feature_names = pickle.load(fd)\n",
    "\n",
    "labels = matrix[:, 1].toarray().astype(int)\n",
    "x = matrix[:, 2:]\n",
    "\n",
    "predictions_by_class = model.predict_proba(x)\n",
    "predictions = predictions_by_class[:, 1]\n",
    "\n",
    "with Live(\"evaluation\", report=\"html\") as live:\n",
    "\n",
    "    # Use dvclive to log a few simple metrics...\n",
    "    avg_prec = metrics.average_precision_score(labels, predictions)\n",
    "    roc_auc = metrics.roc_auc_score(labels, predictions)\n",
    "    live.log_metric(\"avg_prec\", avg_prec)\n",
    "    live.log_metric(\"roc_auc\", roc_auc)\n",
    "\n",
    "    # ... and plots...\n",
    "    live.log_sklearn_plot(\"roc\", labels, predictions)\n",
    "\n",
    "    # ... but actually it can be done with dumping data points into a file:\n",
    "    # ROC has a drop_intermediate arg that reduces the number of points.\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve.\n",
    "    # PRC lacks this arg, so we manually reduce to 1000 points as a rough estimate.\n",
    "    precision, recall, prc_thresholds = metrics.precision_recall_curve(labels,predictions)\n",
    "    nth_point = math.ceil(len(prc_thresholds) / 1000)\n",
    "    prc_points = list(zip(precision, recall, prc_thresholds))[::nth_point]\n",
    "    prc_file = os.path.join(\"evaluation\", \"plots\", \"prc.json\")\n",
    "    with open(prc_file, \"w\") as fd:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"prc\": [\n",
    "                    {\"precision\": p, \"recall\": r, \"threshold\": t}\n",
    "                    for p, r, t in prc_points\n",
    "                ]\n",
    "            },\n",
    "            fd,\n",
    "            indent=4,\n",
    "        )\n",
    "\n",
    "\n",
    "    # ... confusion matrix plot\n",
    "    live.log_sklearn_plot(\"confusion_matrix\",\n",
    "                          labels.squeeze(),\n",
    "                          predictions_by_class.argmax(-1)\n",
    "                         )\n",
    "\n",
    "    # ... and finally, we can dump an image, it's also supported:\n",
    "    fig, axes = plt.subplots(dpi=100)\n",
    "    fig.subplots_adjust(bottom=0.2, top=0.95)\n",
    "    importances = model.feature_importances_\n",
    "    forest_importances = pd.Series(importances, index=feature_names).nlargest(n=30)\n",
    "    axes.set_ylabel(\"Mean decrease in impurity\")\n",
    "    forest_importances.plot.bar(ax=axes)\n",
    "    fig.savefig(os.path.join(\"evaluation\", \"plots\", \"importance.png\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
